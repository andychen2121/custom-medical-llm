{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14abc76c-a9f1-4bc7-b88a-760bf8096a3f",
   "metadata": {},
   "source": [
    "# Custom Medical VQA Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b4bfe2-fce0-4702-a803-01db9dcab7eb",
   "metadata": {},
   "source": [
    "### Goal: \n",
    "Using pre-trained LLM and scraped caption : image dataset, train a VQA model to accurately answer questions based on medical textbook. Explore whether an encoder or decoder model is more appropriate.\n",
    "\n",
    "### Textbook Source:\n",
    "https://drive.google.com/drive/u/2/folders/12mL45XMDRSxhkgMH_PIeQAAsAtbv-X2W\n",
    "\n",
    "### TODO:\n",
    "- question forming: {question : answering : image pairings} from scraped dictionary using coordinate classifier (potentially ask SRI experts)\n",
    "- vision + text modalities: use LLM for text and ??? (resnet CV) for image modality (potentially ask SRI experts; maybe ask for online resources if they have any)\n",
    "- Once modalities in place, reference MedBLIP / MedPalm papers + architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84b07d7-2402-4b93-92a3-fc3f6c50d419",
   "metadata": {},
   "source": [
    "### Pairing + Question Forming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed9e36-fd74-4c42-82f7-aa1297e554ed",
   "metadata": {},
   "source": [
    "##### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b5a8e1b-d0d4-4318-8ff0-bbcca35c684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9614c477-ade0-4896-9aba-ac23357981b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Run scrape notebook first\n",
    "PDF_URL = \"General - Mandell - Core Radiology (1e).pdf\"\n",
    "\n",
    "assert os.path.exists(f\"book-scrape/scrape_out/{PDF_URL.split('.pdf')[0]}\")\n",
    "TEXT_DATA_FOLDER_URL = f\"book-scrape/scrape_out/{PDF_URL.split('.pdf')[0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2f064610-6b61-436b-8b56-759beed48f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.DataFrame()\n",
    "\n",
    "for ch_num, fjson in enumerate(os.listdir(TEXT_DATA_FOLDER_URL)):\n",
    "    ftype = fjson.split('.')[-1]\n",
    "    if ftype != 'json':\n",
    "        continue\n",
    "    fpath = TEXT_DATA_FOLDER_URL + f'/{fjson}'\n",
    "\n",
    "    raw_file_data = pd.read_json(fpath)\n",
    "    raw_file_data['ch'] = [ch_num] * len(raw_file_data)\n",
    "    raw_data = pd.concat([raw_data, raw_file_data])\n",
    "\n",
    "raw_data = raw_data.set_index(['ch'])\n",
    "raw_data = raw_data.sort_index()[['header', 'body', 'images',\n",
    "                                  'label_range', 'pg_range']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec11c048-7e0f-4d0a-94b0-528172538587",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e93ae8-f99d-4670-a788-9fd141e41c37",
   "metadata": {},
   "source": [
    "##### QF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d649681-8c73-482b-b749-f5c0e6924db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how? reference SRI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b5a871-8460-404a-b3cc-3a8e9a6911a3",
   "metadata": {},
   "source": [
    "### Text & Vision Modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94647f3e-c407-48ce-b950-79fe3ee9138e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8115ad58",
   "metadata": {},
   "source": [
    "### Appendix / Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c3be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x-y classification\n",
    "\n",
    "def classify(row):\n",
    "    # basic approach: find center of image, and get closest relevant text or N/A\n",
    "    # returns new df indexed by ???? with text and relevant image\n",
    "    # possibly add radius check; if text within certain radius, it gets added to df\n",
    "    image_coords = np.array(row['image_coords'])\n",
    "    text_coords = np.array(row['text_coords'])\n",
    "    return_df = pd.DataFrame(['image_ref', 'header', 'text'])\n",
    "\n",
    "    # potentially use a kd tree, but n*m should be good enough for 2-dimensions\n",
    "    for img_ref, image_coord_ls in enumerate(image_coords):\n",
    "        img_center = (image_coord_ls[0] + image_coord_ls[1])/2\n",
    "        min_dist = 10000\n",
    "        min_text_ref = -1\n",
    "        for text_ref, text_coord_ls in enumerate(text_coords):\n",
    "            text_center = text_coord_ls[0]\n",
    "            dist = np.sqrt(np.sum((img_center - text_center) ** 2))\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                min_text_ref = text_ref\n",
    "        entry = pd.DataFrame({'image_ref': row['images'][image_ref], \n",
    "                              'header': row['header'], \n",
    "                              'text': row['body']})\n",
    "        return_df = pd.concat(return_df, entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb5a5c6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
